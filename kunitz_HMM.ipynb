{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b209949d",
   "metadata": {},
   "source": [
    "### Retrieving structural data from the PDB using GraphQL API\n",
    "\n",
    "Prior to running this script, an advanced search was performed on the RCSB Protein Data Bank (PDB), filtering for structures containing the Kunitz-type protease inhibitor domain (Pfam: PF00014), with resolution ≤ 3.5 Å and sequence length between 45 and 80 amino acids. From the *Custom Report* interface, a GraphQL query was generated based on this filtered set, yielding a list of 159 PDB IDs.\n",
    "\n",
    "This Python script takes that list of PDB IDs and submits it in batches of 100 to the RCSB GraphQL API. For each structure, it retrieves:\n",
    "- Entry-level resolution data  \n",
    "- Polymer chain identifiers  \n",
    "- Canonical amino acid sequences  \n",
    "- Pfam domain annotations  \n",
    "\n",
    "The output is saved to a JSON file (`all_kunitz_structures.json`), which consolidates all retrieved metadata and sequences for use in downstream filtering, alignment, and HMM construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f59aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# === PARAMETRI ===\n",
    "pdb_ids = [\n",
    "    \"1AAL\", \"1AAP\", \"1B0C\", \"1BHC\", \"1BPI\", \"1BPT\", \"1BRC\", \"1BTH\", \"1BTI\", \"1BUN\", \"1BZ5\", \"1BZX\", \"1CA0\", \"1CBW\", \"1D0D\", \"1DTX\",\n",
    "    \"1EAW\", \"1EJM\", \"1F5R\", \"1F7Z\", \"1FAK\", \"1FAN\", \"1FY8\", \"1G6X\", \"1K6U\", \"1KNT\", \"1KTH\", \"1MTN\", \"1NAG\", \"1P2I\", \"1P2J\", \"1P2K\",\n",
    "    \"1P2M\", \"1P2N\", \"1P2O\", \"1P2Q\", \"1QLQ\", \"1T7C\", \"1T8L\", \"1T8M\", \"1T8N\", \"1T8O\", \"1TAW\", \"1TFX\", \"1TPA\", \"1Y62\", \"1YC0\", \"1YKT\",\n",
    "    \"1YLC\", \"1YLD\", \"1ZJD\", \"1ZR0\", \"2FI3\", \"2FI4\", \"2FI5\", \"2FTL\", \"2FTM\", \"2HEX\", \"2IJO\", \"2KAI\", \"2KNT\", \"2ODY\", \"2PTC\", \"2R9P\",\n",
    "    \"2RA3\", \"2TGP\", \"2TPI\", \"2ZJX\", \"2ZVX\", \"3BTD\", \"3BTE\", \"3BTF\", \"3BTG\", \"3BTH\", \"3BTK\", \"3BTM\", \"3BTQ\", \"3BTT\", \"3BTW\", \"3BYB\",\n",
    "    \"3D65\", \"3FP6\", \"3FP7\", \"3FP8\", \"3GYM\", \"3L33\", \"3LDI\", \"3LDJ\", \"3LDM\", \"3M7Q\", \"3OFW\", \"3OTJ\", \"3P92\", \"3P95\", \"3T62\", \"3TGI\",\n",
    "    \"3TGJ\", \"3TGK\", \"3TPI\", \"3U1J\", \"3UIR\", \"3UOU\", \"3WNY\", \"4BQD\", \"4DG4\", \"4DTG\", \"4ISL\", \"4ISN\", \"4ISO\", \"4NTW\", \"4NTX\", \"4NTY\",\n",
    "    \"4PTI\", \"4TPI\", \"4U30\", \"4U32\", \"4WWY\", \"4WXV\", \"4Y0Y\", \"4Y0Z\", \"4Y10\", \"4Y11\", \"5JB4\", \"5JB5\", \"5JB6\", \"5JB7\", \"5JBT\", \"5M4V\",\n",
    "    \"5NX1\", \"5NX3\", \"5PTI\", \"5XX2\", \"5XX3\", \"5XX4\", \"5XX5\", \"5XX6\", \"5XX7\", \"5XX8\", \"5YV7\", \"5YVU\", \"5YW1\", \"5ZJ3\", \"6BX8\", \"6F1F\",\n",
    "    \"6HAR\", \"6KZF\", \"6PTI\", \"6Q61\", \"6Q6C\", \"6YHY\", \"7PH1\", \"7PTI\", \"7QIQ\", \"7QIR\", \"7QIS\", \"7QIT\", \"8PTI\", \"8VC3\", \"9PTI\"\n",
    "]\n",
    "\n",
    "# === Funzione per dividere la lista in blocchi da 100 ===\n",
    "def chunk_list(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i + size]\n",
    "\n",
    "# === Endpoint GraphQL ===\n",
    "url = \"https://data.rcsb.org/graphql\"\n",
    "\n",
    "# === Struttura della query GraphQL ===\n",
    "def make_query(entry_ids):\n",
    "    ids_formatted = \",\".join(f'\"{pid}\"' for pid in entry_ids)\n",
    "    return {\n",
    "        \"query\": f\"\"\"\n",
    "        {{\n",
    "          entries(entry_ids: [{ids_formatted}]) {{\n",
    "            rcsb_id\n",
    "            rcsb_entry_info {{\n",
    "              resolution_combined\n",
    "            }}\n",
    "            polymer_entities {{\n",
    "              entity_poly {{\n",
    "                pdbx_seq_one_letter_code_can\n",
    "                rcsb_sample_sequence_length\n",
    "              }}\n",
    "              rcsb_polymer_entity_annotation {{\n",
    "                annotation_id\n",
    "                type\n",
    "              }}\n",
    "              polymer_entity_instances {{\n",
    "                rcsb_polymer_entity_instance_container_identifiers {{\n",
    "                  auth_asym_id\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "# === Inizializza il contenitore per tutte le risposte ===\n",
    "all_entries = []\n",
    "\n",
    "# === Esegui la query per ogni batch ===\n",
    "for idx, chunk in enumerate(chunk_list(pdb_ids, 100), start=1):\n",
    "    print(f\" Downloading batch {idx} with {len(chunk)} IDs...\")\n",
    "    response = requests.post(url, json=make_query(chunk))\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        entries = result.get(\"data\", {}).get(\"entries\", [])\n",
    "        all_entries.extend(entries)\n",
    "        time.sleep(1)  # un attimo di pausa tra richieste\n",
    "    else:\n",
    "        print(f\" Errore batch {idx}:\", response.status_code, response.text)\n",
    "\n",
    "# === Salva il file finale ===\n",
    "with open(\"all_kunitz_structures.json\", \"w\") as f:\n",
    "    json.dump({\"entries\": all_entries}, f, indent=2)\n",
    "\n",
    "print(f\"\\n Salvato: all_kunitz_structures.json con {len(all_entries)} strutture.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db1f6",
   "metadata": {},
   "source": [
    "### Filtering and exporting Kunitz sequences in FASTA format\n",
    "\n",
    "To prepare the retrieved dataset for downstream analysis, we developed a second script (`filter_kunitz_to_fasta.py`) that parses the `all_kunitz_structures.json` file and extracts only high-quality, non-redundant sequences matching the desired structural and domain criteria.\n",
    "\n",
    "The script applies the following filters:\n",
    "1. The sequence must contain the Pfam domain **PF00014**\n",
    "2. The associated structure must have an experimental resolution of **≤ 3.5 Å**\n",
    "3. The amino acid sequence must be **between 45 and 80 residues** in length\n",
    "4. The sequence must be **unique** (no duplicates allowed)\n",
    "\n",
    "For each entry passing all criteria, the script extracts the canonical amino acid sequence and writes it to a FASTA-formatted file (`filtered_kunitz_sequences.fasta`). Each header line includes metadata such as the PDB ID, chain identifier(s), resolution, and sequence length.\n",
    "\n",
    "This filtered and deduplicated FASTA file serves as the input for downstream clustering with **CD-HIT**, alignment, and HMM construction.\n",
    "\n",
    "We retrieved 85 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === PARAMETRI ===\n",
    "input_file = \"all_kunitz_structures.json\"\n",
    "output_file = \"filtered_kunitz_sequences.fasta\"\n",
    "min_length = 45\n",
    "max_length = 80\n",
    "max_resolution = 3.5\n",
    "required_pfam = \"PF00014\"\n",
    "\n",
    "# === CARICA IL FILE JSON ===\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "entries = data.get(\"entries\", [])\n",
    "seen_sequences = set()\n",
    "count = 0\n",
    "\n",
    "# === CREA FILE FASTA ===\n",
    "with open(output_file, \"w\") as fasta:\n",
    "    for entry in entries:\n",
    "        pdb_id = entry.get(\"rcsb_id\", \"N/A\")\n",
    "        resolutions = entry.get(\"rcsb_entry_info\", {}).get(\"resolution_combined\", [])\n",
    "        resolution = resolutions[0] if resolutions else None\n",
    "        if resolution is None or resolution > max_resolution:\n",
    "            continue\n",
    "\n",
    "        for entity in entry.get(\"polymer_entities\", []):\n",
    "            sequence = entity.get(\"entity_poly\", {}).get(\"pdbx_seq_one_letter_code_can\", \"\").strip()\n",
    "            length = entity.get(\"entity_poly\", {}).get(\"rcsb_sample_sequence_length\", 0)\n",
    "\n",
    "            # Controllo lunghezza e presenza sequenza\n",
    "            if not sequence or sequence in seen_sequences:\n",
    "                continue\n",
    "            if not (min_length <= length <= max_length):\n",
    "                continue\n",
    "\n",
    "            # Controllo dominio PF00014\n",
    "            annotations = entity.get(\"rcsb_polymer_entity_annotation\", [])\n",
    "            pfam_ids = [ann.get(\"annotation_id\") for ann in annotations if ann.get(\"type\") == \"Pfam\"]\n",
    "            if required_pfam not in pfam_ids:\n",
    "                continue\n",
    "\n",
    "            seen_sequences.add(sequence)\n",
    "            count += 1\n",
    "\n",
    "            # Catena\n",
    "            chains = [\n",
    "                inst.get(\"rcsb_polymer_entity_instance_container_identifiers\", {}).get(\"auth_asym_id\", \"N/A\")\n",
    "                for inst in entity.get(\"polymer_entity_instances\", [])\n",
    "            ]\n",
    "            chain_info = \",\".join(chains) if chains else \"N/A\"\n",
    "\n",
    "            # Scrivi FASTA\n",
    "            header = f\">seq{count} | PDB:{pdb_id} | Chain:{chain_info} | Res:{resolution} | Len:{length}\"\n",
    "            fasta.write(f\"{header}\\n{sequence}\\n\")\n",
    "\n",
    "print(f\" {count} sequences written to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444cfb1",
   "metadata": {},
   "source": [
    "### CD-HIT clustering, structural alignment and HMM construction\n",
    "\n",
    "The 85 non-identical Kunitz sequences were clustered using CD-HIT at 70% identity to remove redundancy.\n",
    "\n",
    "The resulting 19 representative sequences (from pdb_kunitz_nr.fasta) were extracted from headers to create the PDBeFold input.\n",
    "PDBeFold multiple alignment revealed poor alignment for 1bun:B, 4ntw:B, 4u30:X (RMSD > 1.0) and 4bqd:A (Q-score < 0.7), which were excluded.\n",
    "The remaining 15 structures were exported as a structural MSA (kunitz_15_seq_structural_MSA.fasta).\n",
    "Due to an abnormally long sequence (1dtx:A), we removed it to avoid build errors.\n",
    "\n",
    "The final structural MSA was used to build the HMM with HMMER\n",
    "\n",
    "The final model is based on 14 structurally aligned sequences and covers 58 match states, consistent with the typical size of the Kunitz domain.\n",
    "\n",
    "```bash\n",
    "cd-hit -i filtered_kunitz_sequences.fasta -o pdb_kunitz_nr.fasta -c 0.7 -n 5 \\```\n",
    "\n",
    "\n",
    "awk 'BEGIN{RS=\">\"; ORS=\"\"} NR==1{next} $0 !~ /^PDB:1dtx:A/ {print \">\" $0}' kunitz_15_seq_structural_MSA.fasta > kunitz_clean_14_MSA.fasta\n",
    "\n",
    "\n",
    "grep -c \"^>\" kunitz_clean_14_MSA.fasta\n",
    "\n",
    "\n",
    "hmmbuild kunitz_structural.hmm kunitz_clean_14_MSA.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3821172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questo script carica i risultati di un confronto strutturale (output di PDBeFold)\n",
    "contenuti in un file CSV e identifica eventuali outlier in base a soglie definite\n",
    "per RMSD (Root Mean Square Deviation) e Q-score.\n",
    "\n",
    "In particolare:\n",
    "- I punti con RMSD > 1.0 o Q-score < 0.7 sono considerati outlier.\n",
    "- Viene generato un grafico scatterplot con RMSD sull'asse x e Q-score sull'asse y.\n",
    "- Gli outlier sono evidenziati in rosso, gli altri in blu.\n",
    "- Due linee tratteggiate mostrano le soglie utilizzate per l'identificazione.\n",
    "\n",
    "Il plot risultante aiuta a visualizzare la distribuzione delle strutture\n",
    "e a individuare rapidamente eventuali allineamenti strutturali anomali.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Soglie per individuare outlier\n",
    "rmsd_cutoff = 1.0\n",
    "qscore_cutoff = 0.7\n",
    "\n",
    "# Carica il file CSV corretto\n",
    "df = pd.read_csv(\"pdbe_fold_results_clean.csv\")\n",
    "\n",
    "# Identifica gli outlier\n",
    "df[\"is_outlier\"] = (df[\"RMSD\"] > rmsd_cutoff) | (df[\"Qscore\"] < qscore_cutoff)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.scatterplot(data=df, x=\"RMSD\", y=\"Qscore\", hue=\"is_outlier\", palette={True: \"red\", False: \"blue\"}, s=100)\n",
    "\n",
    "# Linee di soglia\n",
    "plt.axvline(x=rmsd_cutoff, color=\"gray\", linestyle=\"--\", label=f\"RMSD = {rmsd_cutoff}\")\n",
    "plt.axhline(y=qscore_cutoff, color=\"gray\", linestyle=\"--\", label=f\"Q-score = {qscore_cutoff}\")\n",
    "\n",
    "# Etichette\n",
    "plt.title(\"Structural Alignment – RMSD vs Q-score\")\n",
    "plt.xlabel(\"RMSD\")\n",
    "plt.ylabel(\"Q-score\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend(title=\"Outlier\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209947a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "```markdown\n",
    "### Validation Phase: Filtering, Test Set Construction and Performance Evaluation\n",
    "\n",
    "In this phase, we constructed non-redundant positive and negative test sets for evaluating the HMM’s predictive performance using cross-validation on SwissProt data. Redundant sequences were excluded using BLAST, and balanced test sets were created. The model was evaluated with `hmmsearch` and metrics computed with `score.py`.\n",
    "\n",
    "**Steps overview:**\n",
    "\n",
    "1) Create BLAST database from SwissProt  \n",
    "2) Align training sequences against the database  \n",
    "3) Filter redundant hits (≥95% identity, ≥50 aa)  \n",
    "4) Remove redundant sequences from the positive set  \n",
    "5) Extract all SwissProt IDs  \n",
    "6) Extract final Kunitz-positive IDs  \n",
    "7) Generate the list of negative IDs (SwissProt – Kunitz)  \n",
    "8) Shuffle positive and negative ID lists  \n",
    "9) Count total entries for each  \n",
    "10) Split positives into two halves (pos_1, pos_2)  \n",
    "11) Split negatives into two halves (neg_1, neg_2)  \n",
    "12) Extract FASTA sequences using ID lists  \n",
    "13) Run `hmmsearch` on all subsets  \n",
    "14) Generate .class files from output  \n",
    "15) Add missing negatives to .class with default values  \n",
    "16) Merge and sort class files  \n",
    "17) Combine positive and negative .class files into set_1 and set_2  \n",
    "18) Run scoring at threshold 1e-5  \n",
    "19) Scan thresholds from 1e-1 to 1e-30 and log results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420eadb",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "makeblastdb -in all_kunitz_sprot.fasta -dbtype prot -out all_kunitz_sprot ```\n",
    "\n",
    "blastp -query kunitz_clean_14_MSA.fasta -db all_kunitz_sprot -out pdb_kunitz_nr_14.blast -outfmt 6\n",
    "\n",
    "awk '$3 >= 95 && $4 >= 50 {print $2}' pdb_kunitz_nr_14.blast | sort -u > ids_to_remove.txt\n",
    "\n",
    "python filter_fasta_remove_ids.py all_kunitz_sprot.fasta ids_to_remove.ids filtered_kunitz_sprot.fasta\n",
    "\n",
    "grep \">\" uniprot_sprot-2.fasta | cut -d \"|\" -f2 > all_sprot.ids\n",
    "grep \">\" filtered_kunitz_sprot.fasta | cut -d \"|\" -f2 > all_kunitz.id\n",
    "comm -23 <(sort all_sprot.ids) <(sort all_kunitz.id) > negs.id\n",
    "sort -R all_kunitz.id > random_all_kunitz.ids\n",
    "sort -R negs.id > random_negs.id\n",
    "wc -l random_all_kunitz.ids\n",
    "wc -l random_negs.id\n",
    "head -n 194 random_all_kunitz.ids > pos_1.ids\n",
    "tail -n +195 random_all_kunitz.ids > pos_2.ids\n",
    "head -n 286421 random_negs.id > neg_1.ids\n",
    "tail -n +286422 random_negs.id > neg_2.ids\n",
    "\n",
    "python3 extract_fasta_by_ids.py pos_1.ids uniprot_sprot-2.fasta > pos_1.fasta\n",
    "python3 extract_fasta_by_ids.py pos_2.ids uniprot_sprot-2.fasta > pos_2.fasta\n",
    "python3 extract_fasta_by_ids.py neg_1.ids uniprot_sprot-2.fasta > neg_1.fasta\n",
    "python3 extract_fasta_by_ids.py neg_2.ids uniprot_sprot-2.fasta > neg_2.fasta\n",
    "\n",
    "hmmsearch -Z 1000 --max --tblout pos_1.out kunitz_structural.hmm pos_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout pos_2.out kunitz_structural.hmm pos_2.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_1.out kunitz_structural.hmm neg_1.fasta\n",
    "hmmsearch -Z 1000 --max --tblout neg_2.out kunitz_structural.hmm neg_2.fasta\n",
    "\n",
    "grep -v \"^#\" pos_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_1.class\n",
    "grep -v \"^#\" pos_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t1\\t\"$5\"\\t\"$8}' > pos_2.class\n",
    "grep -v \"^#\" neg_1.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_1.class\n",
    "grep -v \"^#\" neg_2.out | awk '{split($1,a,\"|\"); print a[2]\"\\t0\\t\"$5\"\\t\"$8}' > neg_2.class\n",
    "\n",
    "comm -23 <(sort neg_1.ids) <(cut -f1 neg_1.class | sort) > neg_1_missing.ids\n",
    "wc -l neg_1_missing.ids\n",
    "awk '{print $1\"\\t0\\t10.0\\t10.0\"}' neg_1_missing.ids > neg_1_missing.class\n",
    "cat neg_1.class neg_1_missing.class > neg_1_complete.class\n",
    "sort neg_1_complete.class > neg_1.class\n",
    "comm -23 <(sort neg_2.ids) <(cut -f1 neg_2.class | sort) > neg_2_missing.ids\n",
    "wc -l neg_2_missing.ids\n",
    "awk '{print $1\"\\t0\\t10.0\\t10.0\"}' neg_2_missing.ids > neg_2_missing.class\n",
    "cat neg_2.class neg_2_missing.class > neg_2_complete.class\n",
    "sort neg_2_complete.class > neg_2.class\n",
    "cat pos_1.class neg_1.class > set_1.class\n",
    "cat pos_2.class neg_2.class > set_2.class\n",
    "\n",
    "python3 score.py set_1.class 1e-5\n",
    "python3 score.py set_2.class 1e-5\n",
    "for i in $(seq 1 30); do python3 score.py set_1.class 1e-$i >> results_set1.txt; done\n",
    "for i in $(seq 1 30); do python3 score.py set_2.class 1e-$i >> results_set2.txt; done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66702aa5",
   "metadata": {},
   "source": [
    "In the following 3 cells,there are the three scripts mentioned in the previous markdown: filter_fasta_remove_ids.py, extract_fasta_by_ids.py, score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c060293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fasta_remove_ids.py\n",
    "\n",
    "# This script removes sequences from a FASTA file based on a list of sequence IDs.\n",
    "# It is used to filter out redundant sequences (e.g. highly similar to training data) \n",
    "# from a positive test set before evaluating the HMM.\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "\n",
    "fasta_file = sys.argv[1]         # es. all_kunitz_sprot.fasta\n",
    "ids_file = sys.argv[2]           # es. ids_to_remove.ids\n",
    "output_file = sys.argv[3]        # es. filtered_kunitz_sprot.fasta\n",
    "\n",
    "# Carica gli ID da rimuovere\n",
    "with open(ids_file) as f:\n",
    "    ids_to_remove = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "# Filtra le sequenze\n",
    "with open(fasta_file) as fin, open(output_file, \"w\") as fout:\n",
    "    for record in SeqIO.parse(fin, \"fasta\"):\n",
    "        if record.id not in ids_to_remove:\n",
    "            SeqIO.write(record, fout, \"fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# This script extracts sequences from a FASTA file based on a list of target IDs.\n",
    "# It is used to build positive and negative test sets by selecting specific sequences\n",
    "# from the full SwissProt database using predefined ID lists.\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "def get_ids(idlist_file):\n",
    "    with open(idlist_file) as f:\n",
    "        return set(f.read().strip().split('\\n'))\n",
    "\n",
    "def get_seq(pidlist, seqfile):\n",
    "    with open(seqfile) as f:\n",
    "        print_flag = False\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                parts = line.strip().split('|')\n",
    "                pid = parts[1] if len(parts) > 1 else line.strip()[1:]\n",
    "                print_flag = pid in pidlist\n",
    "            if print_flag:\n",
    "                print(line.strip())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    idlist_file = sys.argv[1]\n",
    "    seqfile = sys.argv[2]\n",
    "    pidlist = get_ids(idlist_file)\n",
    "    get_seq(pidlist, seqfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385abdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# This script evaluates HMM classification performance by parsing .class files.\n",
    "# It calculates metrics such as sensitivity, specificity, precision, accuracy, \n",
    "# F1-score, and Matthews Correlation Coefficient (MCC) at a specified E-value threshold.\n",
    "\n",
    "import sys\n",
    "\n",
    "threshold = float(sys.argv[2])\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "with open(sys.argv[1]) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        fields = line.split()\n",
    "        ID = fields[0]\n",
    "        real = int(fields[1])\n",
    "        score = float(fields[2])\n",
    "\n",
    "        pred = 1 if score < threshold else 0\n",
    "\n",
    "        if real == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif real == 1 and pred == 0:\n",
    "            FN += 1\n",
    "        elif real == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif real == 0 and pred == 0:\n",
    "            TN += 1\n",
    "\n",
    "# Calculate metrics\n",
    "total = TP + FP + TN + FN\n",
    "acc = (TP + TN) / total if total > 0 else 0\n",
    "mcc_num = (TP * TN) - (FP * FN)\n",
    "mcc_den = ((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))**0.5\n",
    "mcc = mcc_num / mcc_den if mcc_den != 0 else 0\n",
    "tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"    True Positives (TP): {TP}\")\n",
    "print(f\"    False Positives (FP): {FP}\")\n",
    "print(f\"    True Negatives (TN): {TN}\")\n",
    "print(f\"    False Negatives (FN): {FN}\\n\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.6f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.6f}\")\n",
    "print(f\"True Positive Rate (Sensitivity): {tpr:.6f}\")\n",
    "print(f\"False Positive Rate: {fpr:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35925367",
   "metadata": {},
   "source": [
    "### Two-Fold Cross Validation and ROC Analysis – Commands\n",
    "\n",
    "```bash\n",
    "# Run scoring on both sets at fixed threshold\n",
    "python3 score.py set_1.class 1e-5\n",
    "python3 score.py set_2.class 1e-5\n",
    "\n",
    "# Scan multiple thresholds to find optimal MCC\n",
    "for i in $(seq 1 30); do python3 score.py set_1.class 1e-$i >> results_set1.txt; done\n",
    "for i in $(seq 1 30); do python3 score.py set_2.class 1e-$i >> results_set2.txt; done\n",
    "\n",
    "# Run custom script to generate ROC curve and calculate AUC\n",
    "python3 roc_analysis.py set_1.class\n",
    "python3 roc_analysis.py set_2.class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script plots the Matthews Correlation Coefficient (MCC) as a function of the E-value threshold\n",
    "# for two validation sets. It visualizes how model performance varies with threshold selection and \n",
    "# highlights the chosen optimal threshold (e.g., 1e-5) using a vertical dashed red line.\n",
    "# Useful for comparing MCC trends across thresholds and justifying the selected cutoff in cross-validation.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Threshold ottimale scelto nel report (es. 1e-5)\n",
    "best_threshold = 0,1\n",
    "\n",
    "# Plot con linea rossa verticale per il best threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds1, mcc1, marker='o', label='Validation Set 1')\n",
    "plt.plot(thresholds2, mcc2, marker='s', label='Validation Set 2')\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Chosen threshold = {best_threshold:0.1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Threshold (log scale)')\n",
    "plt.ylabel('Matthews Correlation Coefficient (MCC)')\n",
    "plt.title('MCC vs Threshold with Chosen Cutoff')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script generates side-by-side confusion matrix heatmaps for the two validation sets (Set 1 and Set 2)\n",
    "# evaluated at the selected optimal threshold (0.01). Each matrix summarizes the number of true positives (TP),\n",
    "# false negatives (FN), false positives (FP), and true negatives (TN), providing an intuitive overview of \n",
    "# the model’s classification performance on each subset. The use of color-coded heatmaps helps to highlight \n",
    "# differences in prediction accuracy across sets. The output is also saved as a high-resolution PNG image.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix values\n",
    "# Format: [[TP, FN], [FP, TN]]\n",
    "cm_set1 = np.array([[194, 0],\n",
    "                    [0, 286421]])\n",
    "\n",
    "cm_set2 = np.array([[193, 1],\n",
    "                    [0, 286421]])\n",
    "\n",
    "# Etichette degli assi\n",
    "labels = [\"Positive\", \"Negative\"]\n",
    "\n",
    "# Crea il grafico con 2 subplot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix - Set 1\n",
    "sns.heatmap(cm_set1, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=labels, yticklabels=labels, ax=axs[0])\n",
    "axs[0].set_title(\"Confusion Matrix - Set 1 (Threshold = 0.01)\")\n",
    "axs[0].set_xlabel(\"Predicted Label\")\n",
    "axs[0].set_ylabel(\"True Label\")\n",
    "\n",
    "# Confusion matrix - Set 2\n",
    "sns.heatmap(cm_set2, annot=True, fmt=\"d\", cmap=\"Greens\", \n",
    "            xticklabels=labels, yticklabels=labels, ax=axs[1])\n",
    "axs[1].set_title(\"Confusion Matrix - Set 2 (Threshold = 0.01)\")\n",
    "axs[1].set_xlabel(\"Predicted Label\")\n",
    "axs[1].set_ylabel(\"True Label\")\n",
    "\n",
    "# Layout ordinato e salvataggio opzionale\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrices_set1_set2.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script generates a ROC (Receiver Operating Characteristic) curve from a .class file containing \n",
    "# classification results from HMMER (sequence ID, true class label, and E-values). \n",
    "# It transforms E-values into confidence scores using the negative logarithm, computes the true positive \n",
    "# rate (TPR) and false positive rate (FPR) at varying thresholds, and calculates the area under the curve (AUC).\n",
    "# The resulting ROC plot is saved as a PNG image named after the input file.\n",
    "# Usage: python3 plot_roc_curve.py set_1.class\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Check argomenti ---\n",
    "if len(sys.argv) != 2:\n",
    "    print(\"Usage: python3 plot_roc_curve.py <set_file.class>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "input_file = sys.argv[1]\n",
    "\n",
    "# Estrai nome base (es: set_1) per salvare immagine\n",
    "base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "output_image = f\"roc_curve_{base_name}.png\"\n",
    "\n",
    "# --- Carica i dati ---\n",
    "df = pd.read_csv(input_file, sep=\"\\t\", header=None, names=[\"ID\", \"Class\", \"Evalue1\", \"Evalue2\"])\n",
    "\n",
    "# Trasforma E-value in score logaritmico (più basso = predizione migliore)\n",
    "df[\"score\"] = -np.log10(df[\"Evalue1\"].astype(float))\n",
    "\n",
    "# Calcola ROC\n",
    "fpr, tpr, thresholds = roc_curve(df[\"Class\"], df[\"score\"])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# --- Plot ROC ---\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve for {base_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Salva immagine\n",
    "plt.savefig(output_image, dpi=300)\n",
    "print(f\"ROC curve saved to {output_image}\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
